\section{Introduction}

In the past decades, people have invented so many tools to exploit parallelism in processor architectures, programming languages, programming frameworks and runtime.  By Moore's law, hardware are made so cheap that we can literally buy as many hardware as we want and push their computing power to the limit. Inside the processor there is SMT that can execute multiple threads at a time on a single core. Superscalar machines make use of ILP so that independent instructions can be dynamically reordered and executed at once. Parallel programming languages such as Haskell and Cilk provide handy utilities for programmers to write parallel code. 

Therefore, it is enticing to write parallel programs on essentially unlimited hardware and expect a perfect speed up. Nevertheless, according to Amdahl's law, the maximum parallel speed up is often limited by the serial portion. In other words, parallel benefits claimed in theory may not be always achievable. This fact poses a big challenge for the programmer as well as system architect to create their parallel programs and computer systems. And it is undeniable that parallel programming is hard, because everyone strives to find as many parallelism as possible in order to make programs ``faster''. Typically, a parallel call is more costly because the large overhead comes from scheduling, storage management and data transfer. In addition, mutual exclusion ensures serialization that forces critical sections to be executed in sequential. Although there have been too many emphasis on parallel programming, parallelism doesn't come for free. Sometimes the total time executing parallel works is more than doing so sequentially. Then, we get to decide when to actually run tasks in parallel or in sequential. The idea of lazy threading which turns an eager parallel call into a so called parallel ready sequential was put forth by Goldstein et al. in 1996. Parallel calls can be done sequentially when parallelism is not required.

Because parallel programming is hard, but people still need to leverage powerful hardware to do efficient computations, the problem of reducing overhead becomes significantly crucial. Note that it is not always the case where plethora parallelism is necessary and such parallel calls could be better executed in sequential. Hence, our goal is to support an unrestricted parallel thread model and to match the cost of thread creation, scheduling and termination with the cost of sequentially call. So that we would rather execute parallel calls sequentially when parallelism is not needed. 

Hence we introduce user-level interrupts which allows the interrupt to be directly delivered to the user process, bypassing the kernel. We design it such that explicit inter-core communication can be made cheap and easy to program. Given the current architectures where shared memory models and message passing models are predominant, user-level interrupt can show great performance in both worlds. We claim that by using ULI to implement lazy threads with stacklets, we can achieve matched performance when parallelism is not needed.

In addition, it's unavoidable for parallel programs to spend a significant amount of the time in synchronization to ensure the mutual exclusion and correctness. Such latency may quickly become parallel bottleneck and therefore needs further optimization. Because inter-core communication by ULI is cheap, we claim that using ULI on thread synchronization could reduce the overhead. That said, we have implemented our ULI-based locks that reduce cache misses and achieve higher throughput. However, using such synchronization primitives does not put any extra burden on programmers writing their code.

This paper is organized as the follows. In section 2, we discuss related works in terms of reducing parallelism, ULI and locks. In section 3, we present our ULI design on our machine. In section 4, we present two use cases of ULI. In section 5, we further demonstrate our architectural implications. In section 6, we run benchmark and evaluate the outcome. Finally, section 7 is the conclusion.
\lipsum
